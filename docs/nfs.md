# Slurm NFS

Slurm cluster configuration for NFS filesystems

- [Slurm NFS](#slurm-nfs)
  - [Introduction](#introduction)
  - [Configuring NFS shares from the Slurm control node](#configuring-nfs-shares-from-the-slurm-control-node)
    - [Exports from the Slurm control node](#exports-from-the-slurm-control-node)
    - [NFS mounts on the clients](#nfs-mounts-on-the-clients)
  - [Configuring a separate NFS server](#configuring-a-separate-nfs-server)
  - [Disabling NFS](#disabling-nfs)

## Introduction

Slurm clusters typically depend on the presence of one or more shared filesystems, mounted on all the nodes in the cluster.
Having a shared filesystem simplifies software installation and provides a common working space for user jobs,
and many common HPC applications depend on the presence of such a filesystem.

Our default configuration in DeepOps achieves this by configuring the Slurm control/login node as an NFS server,
and the compute nodes as clients of the NFS server.

## Configuring NFS shares from the Slurm control node

By default, we configure two NFS exports from the control node to the compute nodes:

- `/home`: The user home directory space is shared across all nodes in the cluster.
  This is a common pattern on most HPC clusters.

If you would like to make changes to that configuration, you can do so be setting the following variables.

### Exports from the Slurm control node

```yaml
- role: 'nfs-server'
  NFS_EXPORTS:
    - path: '<absolute path of exported directory>' # ex: /home/
      clients: <groups of nodes> # ex: "{{groups.slurm_worker + groups.slurm_submit|default([])}}"

```

You can add as many additional exports to the list as you wish, configuring each appropriately.

The `options` field for each export, which specifies the IPs allowed to mount these exports and the options for the export, follows the format of the NFS `/etc/exports` file.
For documentation on the available NFS export options, see the manpages for your Linux distribution: `man 5 exports`.

### NFS mounts on the clients

```yaml
- role: 'nfs-client'
  NFS_MOUNTS:
    - fs: '<the NFS server directory>' #ex: {{groups.slurm_master[0]}}:/home'
      mountpoint: '<mount point on the NFS client node>' #ex: /home
      options: <option for mounting> # ex: 'rw,defaults'
```

As above, you can add as many additional mounts to the list as you wish.

The `options` field for each mount specifies the NFS options used to mount the filesystem.
For the available NFS options, see the manpages for your Linux distribution: `man 5 nfs`.

You can check at `fstab` stands for "File System Table"
```
sudo cat /etc/fstab
```
Example at worker
```
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
# / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation
/dev/disk/by-id/dm-uuid-LVM-pwWyBhTK6F3BHVTCx4Rjq5997HsNbhCWhba70U5eQPv4dlHnuhz2jJO3SilTsrNj / ext4 defaults 0 1
# /boot was on /dev/sda2 during curtin installation
/dev/disk/by-uuid/8d4e9d42-3448-4e1c-92c5-e9ba12839d55 /boot ext4 defaults 0 1
# /boot/efi was on /dev/sda1 during curtin installation
/dev/disk/by-uuid/AA2A-AF3A /boot/efi vfat defaults 0 1
/swap.img       none    swap    sw      0       0
#VAGRANT-BEGIN
# The contents below are automatically generated by Vagrant. Do not modify.
#VAGRANT-END
controller:/home /home nfs rw,async 0 0
```

## Configuring a separate NFS server

If your site already has an NFS server like NAS device (Synology, Asustor,...). It shoule be configured via the UI to allow specific IP addresses can be mounted to the NFS server provided by NAS. You may wish to use your existing server rather than setting up the Slurm control node to serve NFS.
To configure DeepOps to use your existing server, you should set the following configuration values:

- Set ` hosts` to `all` in the yml file `river_cluster.yml`
- Configure the `nfs_mounts` variable as shown below, repeating the list item for each NFS export

```yaml
nfs_mounts:
  - mountpoint: "<absolute path of directory to mount share on clients>"
    fs: "<hostname of NFS server>:<mount point>"
    options: "<nfs mount options>"
  <Add more here>
```
